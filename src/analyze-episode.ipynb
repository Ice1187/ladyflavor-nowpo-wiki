{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33fc8598-2d4a-4485-8de2-0222f2448ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pprint import pp\n",
    "from collections import Counter\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d515deb-5d1d-4fc0-b842-f8d4b8d6e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, UpdateOne\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"podcast_db\"]  # Database name\n",
    "episodes_collection = db[\"episodes\"]  # Collection for episodes\n",
    "search_index_collection = db[\"search_index\"]  # Collection for full-text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "484b2f61-48b1-4fe3-acc4-c3aab96e4990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../tokenized_tw_transcripts/EP6 罰站就是要背99乘法表！.txt',\n",
      " '../tokenized_tw_transcripts/EP60_1 面試者：噢我的缺點就是真的太瘦了～.txt',\n",
      " '../tokenized_tw_transcripts/EP60_2 對不起我洗完臉沒把毛巾擰乾我真該死嗚嗚嗚.txt',\n",
      " '../tokenized_tw_transcripts/EP61 我是國小潔牙小隊長也要告訴你嗎!?.txt',\n",
      " '../tokenized_tw_transcripts/EP62 不要再玩詐騙手遊了！.txt',\n",
      " '../tokenized_tw_transcripts/EP63 生命中有太多大便了！.txt',\n",
      " '../tokenized_tw_transcripts/EP64 以前的MSN狀態真的太恥啊啊啊啊！本集攝影師老婆代班.txt',\n",
      " '../tokenized_tw_transcripts/EP65 FB廢片的正確使用方式！？.txt',\n",
      " '../tokenized_tw_transcripts/EP66 吸管跟免洗筷最討厭了哼！.txt',\n",
      " '../tokenized_tw_transcripts/EP67 跟阿公去了超奇幻遊覽車進香團？.txt',\n",
      " '../tokenized_tw_transcripts/EP68 漂泊的童年與吃餿水的故事!?!?｜脆脆漂泊童年第１集.txt',\n",
      " '../tokenized_tw_transcripts/EP69 我是傳說中的桃園天才小學生｜脆脆漂泊童年第２集.txt']\n"
     ]
    }
   ],
   "source": [
    "EPISODE_FILES = [str(f) for f in d.glob('EP6*.txt')] \n",
    "                #[str(f) for f in d.glob('EP2*.txt')] + \\\n",
    "                #[str(f) for f in d.glob('EP3*.txt')]\n",
    "EPISODE_FILES = sorted(EPISODE_FILES)\n",
    "pp(EPISODE_FILES)\n",
    "\n",
    "#EPISODE_FILES = [\n",
    "#    '../transcripts/EP60_1 面試者：噢我的缺點就是真的太瘦了～.txt',\n",
    "#    '../transcripts/EP60_2 對不起我洗完臉沒把毛巾擰乾我真該死嗚嗚嗚.txt',\n",
    "#    '../transcripts/EP61 我是國小潔牙小隊長也要告訴你嗎!?.txt',\n",
    "#    '../transcripts/EP62 不要再玩詐騙手遊了！.txt',\n",
    "#    '../transcripts/EP63 生命中有太多大便了！.txt',\n",
    "#    '../transcripts/EP64 以前的MSN狀態真的太恥啊啊啊啊！本集攝影師老婆代班.txt',\n",
    "#    '../transcripts/EP65 FB廢片的正確使用方式！？.txt',\n",
    "#    '../transcripts/EP66 吸管跟免洗筷最討厭了哼！.txt',\n",
    "#    '../transcripts/EP67 跟阿公去了超奇幻遊覽車進香團？.txt',\n",
    "#    '../transcripts/EP68 漂泊的童年與吃餿水的故事!?!?｜脆脆漂泊童年第１集.txt',\n",
    "#    '../transcripts/EP69 我是傳說中的桃園天才小學生｜脆脆漂泊童年第２集.txt',\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b61d109d-f6c5-4ed6-a53a-b4bdb42f8d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP6 罰站就是要背99乘法表！ done\n",
      "EP60_1 面試者：噢我的缺點就是真的太瘦了～ done\n",
      "EP60_2 對不起我洗完臉沒把毛巾擰乾我真該死嗚嗚嗚 done\n",
      "EP61 我是國小潔牙小隊長也要告訴你嗎!? done\n",
      "EP62 不要再玩詐騙手遊了！ done\n",
      "EP63 生命中有太多大便了！ done\n",
      "EP64 以前的MSN狀態真的太恥啊啊啊啊！本集攝影師老婆代班 done\n",
      "EP65 FB廢片的正確使用方式！？ done\n",
      "EP66 吸管跟免洗筷最討厭了哼！ done\n",
      "EP67 跟阿公去了超奇幻遊覽車進香團？ done\n",
      "EP68 漂泊的童年與吃餿水的故事!?!?｜脆脆漂泊童年第１集 done\n",
      "EP69 我是傳說中的桃園天才小學生｜脆脆漂泊童年第２集 done\n",
      "[['嗨', '大家', '好', '我', '是', '翠翠', '歡迎', '來到', '好味', '小姐', '開束', 'wszystk', '我', '要', '還', '你', '原型', '沒有', '要', '對不起'], ['沒有', '要', '我', '還', '以為', '你', '終於', '念對', '了'], ['我', '剛剛', '本來', '有', '一點', '小', '得意', '的'], ['那', '你', '再', '念', '一', '次', '好了'], ['好味', '小姐', '開束', 'wszystk', '我', '還', '你', '原型', '好', '可以', '可以'], ['喔', '唸完', '了'], ['我', '發現', '你們', '前面', '都', '忘記', '自我', '介紹', '了'], ['那', '自我', '介紹', '就', '被', '你', '打斷', '了'], ['沒有', '錯', '就是', '為什麼', '我', '會', '沒有', '自我', '介紹'], ['而且', '然後', '人家', '在', '開頭', '之前', '就', '先', '自我', '介紹']]\n",
      "['嗨', '大家', '好', '我', '是', '翠翠', '歡迎', '來到', '好味', '小姐', '開束', 'wszystk', '我', '要', '還', '你', '原型', '沒有', '要', '對不起', '沒有', '要', '我', '還', '以為', '你', '終於', '念對', '了', '我']\n"
     ]
    }
   ],
   "source": [
    "episodes = []\n",
    "\n",
    "for episode_file in EPISODE_FILES:\n",
    "    episode_name = episode_file[episode_file.rfind('/')+1:-4]\n",
    "    with open(episode_file, 'r') as f:\n",
    "        sentences = f.read().strip().split('\\n')\n",
    "    tokenized_sentences = [[word for word in sentence.split(' ') if word] for sentence in sentences]\n",
    "    tokenized_words = [word for sentence in tokenized_sentences for word in sentence]\n",
    "    word_set = set(tokenized_words)\n",
    "\n",
    "    ep_num_part = episode_name[2:episode_name.find(' ')]\n",
    "    if ep_num_part.find('_') != -1:\n",
    "        ep_num = int(ep_num_part[:ep_num_part.find('_')])\n",
    "        ep_part = int(ep_num_part[ep_num_part.find('_')+1:])\n",
    "    else:\n",
    "        ep_num = int(ep_num_part)\n",
    "        ep_part = None\n",
    "\n",
    "    episode = {\n",
    "        'ep_num': ep_num,\n",
    "        'ep_part': ep_part,\n",
    "        'sentences': tokenized_sentences,\n",
    "        'words': tokenized_words,\n",
    "        'word_set': word_set\n",
    "    }\n",
    "    episodes.append(episode)\n",
    "    print(episode_name, 'done')\n",
    "    \n",
    "print(episodes[0]['sentences'][:10])\n",
    "print(episodes[0]['words'][:30])\n",
    "#print(episodes[0]['word_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fb50a8a-aab8-4e4b-b0b5-ff2e45f0ca40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 186203\n",
      "Unique words: 11530\n"
     ]
    }
   ],
   "source": [
    "s = set()\n",
    "for ep in episodes:\n",
    "    s.update(ep['word_set'])\n",
    "    \n",
    "print('Total words:', sum([len(ep['words']) for ep in episodes]))\n",
    "print('Unique words:', len(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4efa7a-9096-44f4-906a-4e69e771d196",
   "metadata": {},
   "source": [
    "### Build Stopwords List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fa34bf8-5c93-4217-be8d-96a0a588809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    '的', '你', '我', '他', '她', '它', '牠', '就', '是', '個', '了', '這', '那', '對', '有', '一', '不', '啊', '在', '說',\n",
    "    '很','都','喔','要','會','嗎','好','人','跟','問','欸','嗯','也','來','還','隻','看','把','小','去','被','講','種', '太',\n",
    "    '到', '吧', '沒', '嘿', '做', '上', '啦', '嘛', '又', '大', '先', '哦', '耶', '像', '給', '下', '幫', '兩', '得', '再',\n",
    "    '呢', '次', '想', '多', '錯', '讓', '每', '誰', '幾', '最','從','四','著','話','差','聊','者','整','點','全','才','事',\n",
    "    '片','五','坐','過','舉','份','怕','但','裡','蠻','超','只','能', '天', '用', '三', '件', '中', '誒',\n",
    "    # 名詞\n",
    "    '時候', '東西', '大家', '事情', '我們', '你們', '他們', '牠們','現在', '裡面', '外面', '自己', '感覺', '今天', '地方',\n",
    "    # 代名詞\n",
    "    '這樣', '那樣', '這邊', '那邊','什麼',\n",
    "    # 動詞\n",
    "    '覺得', '想說','知道', '開始','看到', '比較', '記得', '出來',\n",
    "    # 形容詞\n",
    "    '一些', '沒有','可以','大概', '之類', '一下', '最後', '不會','不錯', '很多', '突然', '真的', '一定', '直接', '非常', '如果', '一樣',\n",
    "    '一點', '特別', '最近', '以前', \n",
    "    # 副詞\n",
    "    '真的', '就是', '可能','已經','之前','之後', '一直', '好像', '不要', '不行', '後來', '有點', '還是', '應該','不然', '結果',\n",
    "    # 連接詞\n",
    "    '然後', '所以', '因為', '但是', '其實', '反正', '而且', '或是',\n",
    "    # 常見詞\n",
    "    '為什麼', '怎麼', '這麼', '那麼', '這樣子', '幹嘛',\n",
    "\n",
    "    # 其他\n",
    "    '留言',\n",
    "]\n",
    "\n",
    "filtered_words = [word for ep in episodes for word in ep['words'] if word not in stopwords and len(word) == 3]\n",
    "word_counts = Counter(filtered_words)\n",
    "\n",
    "#pp(word_counts.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3b953-c31b-4fae-abe8-5414e5a87f15",
   "metadata": {},
   "source": [
    "### Find Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcc1da83-de01-4c61-abcb-3fb4bf8a8dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 6 keywords in episode 6: 打電動, 小時候, 小孩, 說謊, 電視, 作業\n",
      "Top 6 keywords in episode 60: 面試, 公司, 工作, 雜草, 設計, 問題\n",
      "Top 6 keywords in episode 60: 蜈蚣, 蟑螂, 壁虎, 水蟻, 馬路, 尾巴\n",
      "Top 6 keywords in episode 61: 牙齒, 刷牙, 牙刷, 舞龍, 舞獅, 布子\n",
      "Top 6 keywords in episode 62: 遊戲, crush, candy, 抽到, 刷牙, 結牙\n",
      "Top 6 keywords in episode 63: 大便, 褲子, 上廁所, t恤, 班長, 便秘\n",
      "Top 6 keywords in episode 64: 咖啡廳, 告白, 天氣瓶, 南灣, msn, 室友\n",
      "Top 6 keywords in episode 65: 蚊子, 葡萄, 馬蹄, 影片, 牙結屎, 遊戲\n",
      "Top 6 keywords in episode 66: 吸管, 筷子, 文字, 胡椒, 菠菜, 登登\n",
      "Top 6 keywords in episode 67: 阿公, 阿嬤, 奶奶, 平板, 爺爺, 步道\n",
      "Top 6 keywords in episode 68: 哼哼, 哼哼哼哼, 橘子, 桃園, 整理, 丁香\n",
      "Top 6 keywords in episode 69: 金門, 老師, 考卷, 學校, 蛋餅, 比賽\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "sentences = [' '.join([word for word in ep['words'] if word not in stopwords]) for ep in episodes]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Define how many top keywords you want per episode\n",
    "top_n = 6\n",
    "\n",
    "# Iterate over each document (episode)\n",
    "for doc_idx in range(tfidf_matrix.shape[0]):\n",
    "    # Get the row for the document in sparse format\n",
    "    doc = tfidf_matrix[doc_idx]\n",
    "    \n",
    "    # Convert to array and get the top N indices\n",
    "    sorted_indices = doc.toarray()[0].argsort()[::-1]  # Sort by TF-IDF score in descending order\n",
    "    \n",
    "    # Extract the top N words\n",
    "    top_words = [feature_names[i] for i in sorted_indices[:top_n]]\n",
    "    \n",
    "    # Print top keywords for each episode\n",
    "    print(f\"Top {top_n} keywords in episode {episodes[doc_idx]['ep_num']}: {', '.join(top_words)}\")\n",
    "\n",
    "    episodes[doc_idx]['keywords'] = top_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1da45192-4598-497f-8fa6-91612efc2193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['打電動', '小時候', '小孩', '說謊', '電視', '作業']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[0]['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6af338b-778b-4c9b-a5bd-2f0168f32823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6 upserted. 1 document(s) modified.\n",
      "Episode 60 upserted. 1 document(s) modified.\n",
      "Episode 60 upserted. 1 document(s) modified.\n",
      "Episode 61 upserted. 1 document(s) modified.\n",
      "Episode 62 upserted. 1 document(s) modified.\n",
      "Episode 63 upserted. 1 document(s) modified.\n",
      "Episode 64 upserted. 1 document(s) modified.\n",
      "Episode 65 upserted. 1 document(s) modified.\n",
      "Episode 66 upserted. 1 document(s) modified.\n",
      "Episode 67 upserted. 1 document(s) modified.\n",
      "Episode 68 upserted. 1 document(s) modified.\n",
      "Episode 69 upserted. 1 document(s) modified.\n"
     ]
    }
   ],
   "source": [
    "for episode in episodes:\n",
    "    upsert_data = {\n",
    "        'keywords': episode['keywords']\n",
    "    }\n",
    "    result = episodes_collection.update_one(\n",
    "        {\n",
    "            \"episode_number\": episode[\"ep_num\"],\n",
    "            \"episode_part\": episode['ep_part']\n",
    "        },  # Query by episode_number\n",
    "        {\"$set\": upsert_data},  # Set the episode data\n",
    "        upsert=True  # Insert if it does not exist\n",
    "    )\n",
    "    print(f\"Episode {episode['ep_num']} upserted. {result.modified_count} document(s) modified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ab2ac30-e101-41bd-893f-f92747ad3337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: 0.000*\"哼哼\" + 0.000*\"一樣\" + 0.000*\"發現\" + 0.000*\"時間\"\n",
      "Topic 2: 0.006*\"登登\" + 0.005*\"如果\" + 0.004*\"一樣\" + 0.004*\"雨傘\"\n",
      "Topic 3: 0.005*\"牙齒\" + 0.005*\"老師\" + 0.004*\"刷牙\" + 0.004*\"如果\"\n",
      "Topic 4: 0.013*\"吸管\" + 0.009*\"筷子\" + 0.006*\"討厭\" + 0.006*\"上面\"\n",
      "Topic 5: 0.000*\"發現\" + 0.000*\"一樣\" + 0.000*\"一點\" + 0.000*\"如果\"\n",
      "Topic 6: 0.000*\"小孩\" + 0.000*\"一樣\" + 0.000*\"發現\" + 0.000*\"老師\"\n",
      "Topic 7: 0.008*\"影片\" + 0.006*\"OK\" + 0.005*\"問題\" + 0.005*\"蚊子\"\n",
      "Topic 8: 0.000*\"哼哼\" + 0.000*\"不能\" + 0.000*\"一樣\" + 0.000*\"時間\"\n",
      "Topic 9: 0.005*\"一樣\" + 0.005*\"面試\" + 0.005*\"問題\" + 0.005*\"想要\"\n",
      "Topic 10: 0.001*\"哼哼\" + 0.000*\"小時候\" + 0.000*\"如果\" + 0.000*\"發現\"\n",
      "Topic 11: 0.015*\"遊戲\" + 0.005*\"發現\" + 0.005*\"刷牙\" + 0.005*\"一樣\"\n",
      "Topic 12: 0.028*\"哼哼\" + 0.007*\"老師\" + 0.006*\"第一\" + 0.006*\"學校\"\n",
      "Topic 13: 0.016*\"小時候\" + 0.013*\"小孩\" + 0.009*\"打電動\" + 0.008*\"發現\"\n",
      "Topic 14: 0.013*\"金門\" + 0.007*\"印象\" + 0.006*\"一樣\" + 0.005*\"小時候\"\n",
      "Topic 15: 0.012*\"阿公\" + 0.008*\"阿嬤\" + 0.006*\"一樣\" + 0.005*\"奶奶\"\n",
      "Topic 16: 0.009*\"大便\" + 0.008*\"上廁所\" + 0.007*\"問題\" + 0.006*\"褲子\"\n",
      "Topic 17: 0.010*\"蜈蚣\" + 0.009*\"蟑螂\" + 0.006*\"壁虎\" + 0.005*\"登登\"\n",
      "Topic 18: 0.000*\"一樣\" + 0.000*\"如果\" + 0.000*\"問題\" + 0.000*\"發現\"\n",
      "Topic 19: 0.000*\"一樣\" + 0.000*\"小時候\" + 0.000*\"老師\" + 0.000*\"小孩\"\n",
      "Topic 20: 0.001*\"哼哼\" + 0.000*\"如果\" + 0.000*\"遊戲\" + 0.000*\"第一\"\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "texts = [[word for word in ep['words'] if word not in stopwords and len(word) > 1] for ep in episodes]\n",
    "id2word = corpora.Dictionary(texts)\n",
    "\n",
    "# Create a document-term matrix\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# Step 3: Build the LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=20, random_state=42, passes=10)\n",
    "\n",
    "# Step 4: Print the topics with the top words for each topic\n",
    "for idx, topic in lda_model.print_topics(num_words=4):\n",
    "    print(f\"Topic {idx+1}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fd3e19e-1cee-4f0d-bd11-6ab7207f6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
